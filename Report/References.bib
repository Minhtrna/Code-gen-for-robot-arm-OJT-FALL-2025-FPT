@undefined{,}
@techReport{Yujie,
   abstract = {Spiking neural networks (SNNs) that enables energy efficient implementation on emerging neuromorphic hardware are gaining more attention. Yet now, SNNs have not shown competitive performance compared with artificial neural networks (ANNs), due to the lack of effective learning algorithms and efficient programming frameworks. We address this issue from two aspects: (1) We propose a neuron nor-malization technique to adjust the neural selectivity and develop a direct learning algorithm for deep SNNs. (2) Via narrowing the rate coding window and converting the leaky integrate-and-fire (LIF) model into an explicitly iterative version , we present a Pytorch-based implementation method towards the training of large-scale SNNs. In this way, we are able to train deep SNNs with tens of times speedup. As a result, we achieve significantly better accuracy than the reported works on neuromorphic datasets (N-MNIST and DVS-CIFAR10), and comparable accuracy as existing ANNs and pre-trained SNNs on non-spiking datasets (CIFAR10). To our best knowledge, this is the first work that demonstrates direct training of deep SNNs with high performance on CIFAR10, and the efficient implementation provides a new way to explore the potential of SNNs.},
   author = {Yujie Wu and Lei Deng and Guoqi Li and Jun Zhu and Yuan Xie and Luping Shi},
   pages = {19},
   title = {Direct Training for Spiking Neural Networks: Faster, Larger, Better},
   url = {www.aaai.org}
}
@techReport{Byunggook,
   abstract = {Spiking neural networks (SNNs) that mimic information transmission in the brain can energy-efficiently process spatio-temporal information through discrete and sparse spikes, thereby receiving considerable attention. To improve accuracy and energy efficiency of SNNs, most previous studies have focused solely on training methods, and the effect of architecture has rarely been studied. We investigate the design choices used in the previous studies in terms of the accuracy and number of spikes and figure out that they are not best-suited for SNNs. To further improve the accuracy and reduce the spikes generated by SNNs, we propose a spike-aware neural architecture search framework called AutoSNN. We define a search space consisting of architectures without undesirable design choices. To enable the spike-aware architecture search, we introduce a fitness that considers both the accuracy and number of spikes. AutoSNN successfully searches for SNN archi-tectures that outperform hand-crafted SNNs in accuracy and energy efficiency. We thoroughly demonstrate the effectiveness of AutoSNN on various datasets including neuromorphic datasets.},
   author = {Byunggook Na and Jisoo Mok and Seongsik Park and Dongjin Lee and Hyeokjun Choe and Sungroh Yoon},
   title = {AutoSNN: Towards Energy-Efficient Spiking Neural Networks},
   url = {https://github.com/nabk89/AutoSNN.}
}
@article{Ali,
   abstract = {With the rise of Transformers as the standard for language processing, and their advancements in computer vision , there has been a corresponding growth in parameter size and amounts of training data. Many have come to believe that because of this, transformers are not suitable for small sets of data. This trend leads to concerns such as: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field. In this paper, we aim to present an approach for small-scale learning by introducing Compact Transformers. We show for the first time that with the right size, convolu-tional tokenization, transformers can avoid overfitting and outperform state-of-the-art CNNs on small datasets. Our models are flexible in terms of model size, and can have as little as 0.28M parameters while achieving competitive results. Our best model can reach 98% accuracy when training from scratch on CIFAR-10 with only 3.7M parameters, which is a significant improvement in data-efficiency over previous Transformer based models being over 10x smaller than other transformers and is 15% the size of ResNet50 while achieving similar performance. CCT also outper-forms many modern CNN based approaches, and even some recent NAS-based approaches. Additionally, we obtain a new SOTA result on Flowers-102 with 99.76% top-1 accuracy , and improve upon the existing baseline on ImageNet (82.71% accuracy with 29% as many parameters as ViT), as well as NLP tasks. Our simple and compact design for transformers makes them more feasible to study for those with limited computing resources and/or dealing with small datasets, while extending existing research efforts in data efficient transformers. Convolution},
   author = {Ali Hassani and Abulikemu Abuduweili and Steven Walton and Nikhil Shah and Jiachen Li and Humphrey Shi},
   doi = {10.48550/arXiv.2104.05704},
   title = {Escaping the Big Data Paradigm with Compact Transformers},
   url = {https://github.com/SHI-Labs/Compact-Transformers}
}
@article{Qin2024,
   abstract = {We present the latest generation of MobileNets, known as MobileNetV4 (MNv4), featuring universally efficient architecture designs for mobile devices. At its core, we introduce the Universal Inverted Bottleneck (UIB) search block, a unified and flexible structure that merges Inverted Bottleneck (IB), ConvNext, Feed Forward Network (FFN), and a novel Extra Depthwise (ExtraDW) variant. Alongside UIB, we present Mobile MQA, an attention block tailored for mobile accelerators, delivering a significant 39% speedup. An optimized neural architecture search (NAS) recipe is also introduced which improves MNv4 search effectiveness. The integration of UIB, Mobile MQA and the refined NAS recipe results in a new suite of MNv4 models that are mostly Pareto optimal across mobile CPUs, DSPs, GPUs, as well as specialized accelerators like Apple Neural Engine and Google Pixel EdgeTPU - a characteristic not found in any other models tested. Finally, to further boost accuracy, we introduce a novel distillation technique. Enhanced by this technique, our MNv4-Hybrid-Large model delivers 87% ImageNet-1K accuracy, with a Pixel 8 EdgeTPU runtime of just 3.8ms.},
   author = {Danfeng Qin and Chas Leichner and Manolis Delakis and Marco Fornoni and Shixin Luo and Fan Yang and Weijun Wang and Colby Banbury and Chengxi Ye and Berkin Akin and Vaibhav Aggarwal and Tenghui Zhu and Daniele Moro and Andrew Howard},
   month = {4},
   title = {MobileNetV4 -- Universal Models for the Mobile Ecosystem},
   url = {http://arxiv.org/abs/2404.10518},
   year = {2024}
}
@article{Takaghaj2024,
   abstract = {Neuromorphic computing has recently gained momentum with the emergence of various neuromorphic processors. As the field advances, there is an increasing focus on developing training methods that can effectively leverage the unique properties of spiking neural networks (SNNs). SNNs emulate the temporal dynamics of biological neurons, making them particularly well-suited for real-time, event-driven processing. To fully harness the potential of SNNs across different neuromorphic platforms, effective training methodologies are essential. In SNNs, learning rules are based on neurons' spiking behavior, that is, if and when spikes are generated due to a neuron's membrane potential exceeding that neuron's spiking threshold, and this spike timing encodes vital information. However, the threshold is generally treated as a hyperparameter, and incorrect selection can lead to neurons that do not spike for large portions of the training process, hindering the effective rate of learning. This work focuses on the significance of learning neuron thresholds alongside weights in SNNs. Our results suggest that promoting threshold from a hyperparameter to a trainable parameter effectively addresses the issue of dead neurons during training. This leads to a more robust training algorithm, resulting in improved convergence, increased test accuracy, and a substantial reduction in the number of training epochs required to achieve viable accuracy on spatiotemporal datasets such as NMNIST, DVS128, and Spiking Heidelberg Digits (SHD), with up to 30% training speed-up and up to 2% higher accuracy on these datasets.},
   author = {Sanaz Mahmoodi Takaghaj and Jack Sampson},
   month = {7},
   title = {To Spike or Not to Spike, that is the Question},
   url = {http://arxiv.org/abs/2407.19566},
   year = {2024}
}
@techReport{Hao2024,
   abstract = {Compared to traditional Artificial Neural Network (ANN), Spiking Neural Network (SNN) has garnered widespread academic interest for its intrinsic ability to transmit information in a more energy-efficient manner. However, despite previous efforts to optimize the learning algorithm of SNNs through various methods, SNNs still lag behind ANNs in terms of performance. The recently proposed multi-threshold model provides more possibilities for further enhancing the learning capability of SNNs. In this paper, we rigorously analyze the relationship among the multi-threshold model, vanilla spiking model and quantized ANNs from a mathematical perspective, then propose a novel LM-HT model, which is an equidistant multi-threshold model that can dynamically regulate the global input current and membrane potential leakage on the time dimension. The LM-HT model can also be transformed into a vanilla single threshold model through reparameterization, thereby achieving more flexible hardware deployment. In addition, we note that the LM-HT model can seamlessly integrate with ANN-SNN Conversion framework under special initialization. This novel hybrid learning framework can effectively improve the relatively poor performance of converted SNNs under low time latency. Extensive experimental results have demonstrated that our model can outperform previous state-of-the-art works on various types of datasets, which promote SNNs to achieve a brand-new level of performance comparable to quantized ANNs. Code is available at https://github.com/hzc1208/LMHT_SNN.},
   author = {Zecheng Hao and Xinyu Shi and Yujia Liu and Zhaofei Yu and Tiejun Huang},
   title = {LM-HT SNN: Enhancing the Performance of SNN to ANN Counterpart through Learnable Multi-hierarchical Threshold Model},
   url = {https://github.com/hzc1208/LMHT_SNN.},
   year = {2024}
}
@article{Guo2023,
   abstract = {The Spiking Neural Network (SNN) has attracted more and more attention recently. It adopts binary spike signals to transmit information. Benefitting from the information passing paradigm of SNNs, the multiplications of activations and weights can be replaced by additions, which are more energy-efficient. However, its "Hard Reset" mechanism for the firing activity would ignore the difference among membrane potentials when the membrane potential is above the firing threshold, causing information loss. Meanwhile, quantifying the membrane potential to 0/1 spikes at the firing instants will inevitably introduce the quantization error thus bringing about information loss too. To address these problems, we propose to use the "Soft Reset" mechanism for the supervised training-based SNNs, which will drive the membrane potential to a dynamic reset potential according to its magnitude, and Membrane Potential Rectifier (MPR) to reduce the quantization error via redistributing the membrane potential to a range close to the spikes. Results show that the SNNs with the "Soft Reset" mechanism and MPR outperform their vanilla counterparts on both static and dynamic datasets.},
   author = {Yufei Guo and Yuanpei Chen and Liwen Zhang and Xiaode Liu and Xinyi Tong and Yuanyuan Ou and Xuhui Huang and Zhe Ma},
   month = {7},
   title = {InfLoR-SNN: Reducing Information Loss for Spiking Neural Networks},
   url = {http://arxiv.org/abs/2307.04356},
   year = {2023}
}
@article{Shrestha2022,
   abstract = {Spiking Neural Networks~(SNNs) are a promising research paradigm for low power edge-based computing. Recent works in SNN backpropagation has enabled training of SNNs for practical tasks. However, since spikes are binary events in time, standard loss formulations are not directly compatible with spike output. As a result, current works are limited to using mean-squared loss of spike count. In this paper, we formulate the output probability interpretation from the spike count measure and introduce spike-based negative log-likelihood measure which are more suited for classification tasks especially in terms of the energy efficiency and inference latency. We compare our loss measures with other existing alternatives and evaluate using classification performances on three neuromorphic benchmark datasets: NMNIST, DVS Gesture and N-TIDIGITS18. In addition, we demonstrate state of the art performances on these datasets, achieving faster inference speed and less energy consumption.},
   author = {Sumit Bam Shrestha and Longwei Zhu and Pengfei Sun},
   month = {5},
   title = {Spikemax: Spike-based Loss Methods for Classification},
   url = {http://arxiv.org/abs/2205.09845},
   year = {2022}
}
@article{Nunes2022,
   abstract = {The field of Deep Learning (DL) has seen a remarkable series of developments with increasingly accurate and robust algorithms. However, the increase in performance has been accompanied by an increase in the parameters, complexity, and training and inference time of the models, which means that we are rapidly reaching a point where DL may no longer be feasible. On the other hand, some specific applications need to be carefully considered when developing DL models due to hardware limitations or power requirements. In this context, there is a growing interest in efficient DL algorithms, with Spiking Neural Networks (SNNs) being one of the most promising paradigms. Due to the inherent asynchrony and sparseness of spike trains, these types of networks have the potential to reduce power consumption while maintaining relatively good performance. This is attractive for efficient DL and, if successful, could replace traditional Artificial Neural Networks (ANNs) in many applications. However, despite significant progress, the performance of SNNs on benchmark datasets is often lower than that of traditional ANNs. Moreover, due to the non-differentiable nature of their activation functions, it is difficult to train SNNs with direct backpropagation, so appropriate training strategies must be found. Nevertheless, significant efforts have been made to develop competitive models. This survey covers the main ideas behind SNNs and reviews recent trends in learning rules and network architectures, with a particular focus on biologically inspired strategies. It also provides some practical considerations of state-of-the-art SNNs and discusses relevant research opportunities.},
   author = {Joao D. Nunes and Marcelo Carvalho and Diogo Carneiro and Jaime S. Cardoso},
   doi = {10.1109/ACCESS.2022.3179968},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Artificial neural networks,computer vision,efficient deep learning,event-driven,machine learning,neuromorphic computing,neuromorphic hardware,spiking neural networks},
   pages = {60738-60764},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Spiking Neural Networks: A Survey},
   volume = {10},
   year = {2022}
}
@article{Zhou2022,
   abstract = {We consider two biologically plausible structures, the Spiking Neural Network (SNN) and the self-attention mechanism. The former offers an energy-efficient and event-driven paradigm for deep learning, while the latter has the ability to capture feature dependencies, enabling Transformer to achieve good performance. It is intuitively promising to explore the marriage between them. In this paper, we consider leveraging both self-attention capability and biological properties of SNNs, and propose a novel Spiking Self Attention (SSA) as well as a powerful framework, named Spiking Transformer (Spikformer). The SSA mechanism in Spikformer models the sparse visual feature by using spike-form Query, Key, and Value without softmax. Since its computation is sparse and avoids multiplication, SSA is efficient and has low computational energy consumption. It is shown that Spikformer with SSA can outperform the state-of-the-art SNNs-like frameworks in image classification on both neuromorphic and static datasets. Spikformer (66.3M parameters) with comparable size to SEW-ResNet-152 (60.2M,69.26%) can achieve 74.81% top1 accuracy on ImageNet using 4 time steps, which is the state-of-the-art in directly trained SNNs models.},
   author = {Zhaokun Zhou and Yuesheng Zhu and Chao He and Yaowei Wang and Shuicheng Yan and Yonghong Tian and Li Yuan},
   month = {11},
   title = {Spikformer: When Spiking Neural Network Meets Transformer},
   url = {http://arxiv.org/abs/2209.15425},
   year = {2022}
}
@techReport{Meng2022,
   abstract = {Spiking Neural Network (SNN) is a promising energy-efficient AI model when implemented on neuromorphic hardware. However, it is a challenge to efficiently train SNNs due to their non-differentiability. Most existing methods either suffer from high latency (i.e., long simulation time steps), or cannot achieve as high performance as Artificial Neural Networks (ANNs). In this paper, we propose the Differentiation on Spike Representation (DSR) method, which could achieve high performance that is competitive to ANNs yet with low latency. First, we encode the spike trains into spike representation using (weighted) firing rate coding. Based on the spike representation, we systematically derive that the spiking dynamics with common neural models can be represented as some sub-differentiable mapping. With this viewpoint, our proposed DSR method trains SNNs through gradients of the mapping and avoids the common non-differentiability problem in SNN training. Then we analyze the error when representing the specific mapping with the forward computation of the SNN. To reduce such error, we propose to train the spike threshold in each layer, and to introduce a new hyperparameter for the neural models. With these components, the DSR method can achieve state-of-the-art SNN performance with low latency on both static and neuromorphic datasets, including CIFAR-10, CIFAR-100, ImageNet, and DVS-CIFAR10.},
   author = {Qingyan Meng and Mingqing Xiao and Shen Yan and Yisen Wang and Zhouchen Lin and Zhi-Quan Luo},
   title = {Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation},
   year = {2022}
}
@article{Mehta2021,
   abstract = {Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets},
   author = {Sachin Mehta and Mohammad Rastegari},
   month = {10},
   title = {MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer},
   url = {http://arxiv.org/abs/2110.02178},
   year = {2021}
}
@techReport{Li2021,
   abstract = {Spiking neural networks (SNNs) have emerged as a biology-inspired method mimicking the spiking nature of brain neurons. This biomimicry derives SNNs' energy efficiency of inference on neuromorphic hardware. However, it also causes an intrinsic disadvantage in training high-performing SNNs from scratch since the discrete spike prohibits the gradient calculation. To overcome this issue, the surrogate gradient (SG) approach has been proposed as a continuous relaxation. Yet the heuristic choice of SG leaves it vacant how the SG benefits the SNN training. In this work, we first theoretically study the gradient descent problem in SNN training and introduce finite difference gradient to quantitatively analyze the training behavior of SNN. Based on the introduced finite difference gradient, we propose a new family of Differentiable Spike (Dspike) functions that can adaptively evolve during training to find the optimal shape and smoothness for gradient estimation. Extensive experiments over several popular network structures show that training SNN with Dspike consistently outperforms the state-of-the-art training methods. For example, on the CIFAR10-DVS classification task, we can train a spiking ResNet-18 and achieve 75.4% top-1 accuracy with 10 time steps.},
   author = {Yuhang Li and Yufei Guo and Shanghang Zhang and Shikuang Deng and Yongqing Hai and Shi Gu},
   title = {Differentiable Spike: Rethinking Gradient-Descent for Training Spiking Neural Networks},
   url = {https://proceedings.neurips.cc/paper/2021/file/c4ca4238a0b923820dcc509a6f75849b-Paper.pdf},
   year = {2021}
}
@article{Hassani2021,
   abstract = {With the rise of Transformers as the standard for language processing, and their advancements in computer vision , there has been a corresponding growth in parameter size and amounts of training data. Many have come to believe that because of this, transformers are not suitable for small sets of data. This trend leads to concerns such as: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field. In this paper, we aim to present an approach for small-scale learning by introducing Compact Transformers. We show for the first time that with the right size, convolu-tional tokenization, transformers can avoid overfitting and outperform state-of-the-art CNNs on small datasets. Our models are flexible in terms of model size, and can have as little as 0.28M parameters while achieving competitive results. Our best model can reach 98% accuracy when training from scratch on CIFAR-10 with only 3.7M parameters, which is a significant improvement in data-efficiency over previous Transformer based models being over 10x smaller than other transformers and is 15% the size of ResNet50 while achieving similar performance. CCT also outper-forms many modern CNN based approaches, and even some recent NAS-based approaches. Additionally, we obtain a new SOTA result on Flowers-102 with 99.76% top-1 accuracy , and improve upon the existing baseline on ImageNet (82.71% accuracy with 29% as many parameters as ViT), as well as NLP tasks. Our simple and compact design for transformers makes them more feasible to study for those with limited computing resources and/or dealing with small datasets, while extending existing research efforts in data efficient transformers. Convolution},
   author = {Ali Hassani and Abulikemu Abuduweili and Steven Walton and Nikhil Shah and Jiachen Li and Humphrey Shi},
   doi = {10.48550/arXiv.2104.05704},
   title = {Escaping the Big Data Paradigm with Compact Transformers},
   url = {https://github.com/SHI-Labs/Compact-Transformers},
   year = {2021}
}
@article{Zheng2020,
   abstract = {Spiking neural networks (SNNs) are promising in a bio-plausible coding for spatio-temporal information and event-driven signal processing, which is very suited for energy-efficient implementation in neuromorphic hardware. However, the unique working mode of SNNs makes them more difficult to train than traditional networks. Currently, there are two main routes to explore the training of deep SNNs with high performance. The first is to convert a pre-trained ANN model to its SNN version, which usually requires a long coding window for convergence and cannot exploit the spatio-temporal features during training for solving temporal tasks. The other is to directly train SNNs in the spatio-temporal domain. But due to the binary spike activity of the firing function and the problem of gradient vanishing or explosion, current methods are restricted to shallow architectures and thereby difficult in harnessing large-scale datasets (e.g. ImageNet). To this end, we propose a threshold-dependent batch normalization (tdBN) method based on the emerging spatio-temporal backpropagation, termed "STBP-tdBN", enabling direct training of a very deep SNN and the efficient implementation of its inference on neuromorphic hardware. With the proposed method and elaborated shortcut connection, we significantly extend directly-trained SNNs from a shallow structure ( < 10 layer) to a very deep structure (50 layers). Furthermore, we theoretically analyze the effectiveness of our method based on "Block Dynamical Isometry" theory. Finally, we report superior accuracy results including 93.15 % on CIFAR-10, 67.8 % on DVS-CIFAR10, and 67.05% on ImageNet with very few timesteps. To our best knowledge, it's the first time to explore the directly-trained deep SNNs with high performance on ImageNet.},
   author = {Hanle Zheng and Yujie Wu and Lei Deng and Yifan Hu and Guoqi Li},
   month = {12},
   title = {Going Deeper With Directly-Trained Larger Spiking Neural Networks},
   url = {http://arxiv.org/abs/2011.05280},
   year = {2020}
}
@article{Xu2017,
   abstract = {Spiking neural networks (SNNs) can utilize spatio-temporal information and have a nature of energy efficiency which is a good alternative to deep neural networks(DNNs). The event-driven information processing makes SNNs can reduce the expensive computation of DNNs and save a lot of energy consumption. However, high training and inference latency is a limitation of the development of deeper SNNs. SNNs usually need tens or even hundreds of time steps during the training and inference process which causes not only the increase of latency but also the waste of energy consumption. To overcome this problem, we proposed a novel training method based on backpropagation (BP) for ultra-low latency(1-2 time steps) SNN with multi-threshold. In order to increase the information capacity of each spike, we introduce the multi-threshold Leaky Integrate and Fired (LIF) model. In our proposed training method, we proposed three approximated derivative for spike activity to solve the problem of the non-differentiable issue which cause difficulties for direct training SNNs based on BP. The experimental results show that our proposed method achieves an average accuracy of 99.56%, 93.08%, and 87.90% on MNIST, FashionMNIST, and CIFAR10, respectively with only 2 time steps. For the CIFAR10 dataset, our proposed method achieve 1.12% accuracy improvement over the previously reported direct trained SNNs with fewer time steps.},
   author = {Changqing Xu and Yintang Yang and Yi Liu},
   doi = {10.48550/arXiv.2112.07426},
   isbn = {2112.07426v1},
   keywords = {Backpropagation,Index Terms-Spiking neural networks,Multi-threshold,Ultra low latency},
   pages = {1},
   title = {Direct Training via Backpropagation for Ultra-low Latency Spiking Neural Networks with Multi-threshold},
   url = {https://www.researchgate.net/publication/357046308},
   year = {2017}
}
@article{lu2022linear,
  title={Linear leaky-integrate-and-fire neuron model based spiking neural networks and its mapping relationship to deep neural networks},
  author={Lu, Sijia and Xu, Feng},
  journal={Frontiers in neuroscience},
  volume={16},
  pages={857513},
  year={2022},
  publisher={Frontiers Media SA}
}
@article{Wu2018,
   abstract = {Spiking neural networks (SNNs) are promising in ascertaining brain-like behaviors since spikes are capable of encoding spatio-temporal information. Recent schemes, e.g., pre-training from artificial neural networks (ANNs) or direct training based on backpropagation (BP), make the high-performance supervised training of SNNs possible. However, these methods primarily fasten more attention on its spatial domain information, and the dynamics in temporal domain are attached less significance. Consequently, this might lead to the performance bottleneck, and scores of training techniques shall be additionally required. Another underlying problem is that the spike activity is naturally non-differentiable, raising more difficulties in supervised training of SNNs. In this paper, we propose a spatio-temporal backpropagation (STBP) algorithm for training high-performance SNNs. In order to solve the non-differentiable problem of SNNs, an approximated derivative for spike activity is proposed, being appropriate for gradient descent training. The STBP algorithm combines the layer-by-layer spatial domain (SD) and the timing-dependent temporal domain (TD), and does not require any additional complicated skill. We evaluate this method through adopting both the fully connected and convolutional architecture on the static MNIST dataset, a custom object detection dataset, and the dynamic N-MNIST dataset. Results bespeak that our approach achieves the best accuracy compared with existing state-of-the-art algorithms on spiking networks. This work provides a new perspective to investigate the high-performance SNNs for future brain-like computing paradigm with rich spatio-temporal dynamics.},
   author = {Yujie Wu and Lei Deng and Guoqi Li and Jun Zhu and Luping Shi},
   doi = {10.3389/fnins.2018.00331},
   issn = {1662453X},
   issue = {MAY},
   journal = {Frontiers in Neuroscience},
   keywords = {Backpropagation,Convolutional neural networks (CNN),Leaky integrate-and-fire neuron,MNIST,MNIST-DVS,Spatio-temporal recognition,Spiking neural network (SNN)},
   month = {5},
   publisher = {Frontiers Media S.A.},
   title = {Spatio-temporal backpropagation for training high-performance spiking neural networks},
   volume = {12},
   year = {2018}
}
@article{neftci2019surrogate,
  title={Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks},
  author={Neftci, Emre O and Mostafa, Hesham and Zenke, Friedemann},
  journal={IEEE Signal Processing Magazine},
  volume={36},
  number={6},
  pages={51--63},
  year={2019},
  publisher={IEEE}
}
@inproceedings{meng2023towards,
  title={Towards memory-and time-efficient backpropagation for training spiking neural networks},
  author={Meng, Qingyan and Xiao, Mingqing and Yan, Shen and Wang, Yisen and Lin, Zhouchen and Luo, Zhi-Quan},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6166--6176},
  year={2023}
}
@article{li2024directly,
  title={Directly training temporal Spiking Neural Network with sparse surrogate gradient},
  author={Li, Yang and Zhao, Feifei and Zhao, Dongcheng and Zeng, Yi},
  journal={Neural Networks},
  volume={179},
  pages={106499},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={6105--6114},
  year={2019},
  organization={PMLR}
}
@article{NIU2023106322,
title = {Event-driven spiking neural network based on membrane potential modulation for remote sensing image classification},
journal = {Engineering Applications of Artificial Intelligence},
volume = {123},
pages = {106322},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106322},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623005067},
author = {Li-Ye Niu and Ying Wei and Yue Liu},
keywords = {Spiking neural network, Remote sensing images, Transfer learning, Membrane potential modulation, Spike firing rate},
abstract = {Spiking neural network (SNN) based on sparse triggering and event-driven is a hardware-friendly model. SNN can provide an ultra-low power alternative for the deep neural network (DNN) to process remote sensing images. Brain information processing depends on the action potential of neurons. Therefore, the biological rationality of the artificial neural network (ANN) has been questioned. SNN is a more suitable model for brain information processing mechanisms. At present, the SNN obtained by ANN conversion has achieved the best performance in the current image processing tasks. However, the method based on ANN to SNN will have performance loss in the conversion process. Herein, we proposed a spiking neuron threshold-following reset (TF-reset) method and a membrane potential modulation method to reduce the loss of network conversion. We theoretically analyzed the proposed TF-reset and deduced the relationship between spike firing rate and neuron activation. In the experiment, we used an improved VGG-15 architecture combined with the method of transfer learning to apply the model to the classification task of remote sensing images. SNN-VGG-15 based on TF-reset and membrane potential modulation algorithm achieved a classification accuracy of 99.14%, 94.54%, and 95.00% on UCM, RSSCN7, and AID. Our algorithm can not only realize the lossless conversion of SNN but also outperforms the original network in classification performance on UCM and RSSCN7. In addition, our model also has advantages in energy consumption and noise robustness. The algorithm in this paper can provide a reference for the research remote sensing images procession using SNN.}
}