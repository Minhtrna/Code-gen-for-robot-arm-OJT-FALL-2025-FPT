import torch
from torch import nn
import torch.nn.functional as F
from torch.autograd import Function


#===== computes function ===== 

@torch.jit.script
def forward_compute(input: torch.Tensor, thrs: torch.Tensor, num_thrs: int):
    """
    forward pass compute the float output of the neuron which reperesents the
    number of spikes generated by the neuron on different thresholds levels. used a dynamic mask
    Args:
        input (torch.Tensor): The input tensor to the neuron.
        thrs (torch.Tensor): The threshold levels for the neuron.
        num_thrs (int): The number of threshold levels.
        out (torch.Tensor): The output tensor storing the number of spikes.
    Returns:
        out (torch.Tensor): The output tensor with the number of spikes occurred.

    """
    level_outputs = []
    cumulative_mask = torch.zeros_like(input)
    for level in range(num_thrs, 0, -1):
        mask = (input >= level * thrs).float()
        clean_mask = mask * (1. - cumulative_mask)
        level_outputs.append(clean_mask * level * thrs)
        cumulative_mask = cumulative_mask + clean_mask

    out = torch.stack(level_outputs).sum(dim=0)
    return out

@torch.jit.script
def forward_compute_w_mask(input: torch.Tensor, thrs: torch.Tensor, num_thrs: int):
    """
    forward pass compute the float output of the neuron which reperesents the
    number of spikes generated by the neuron on different thresholds levels. used a dynamic mask
    Provide weight mask for gradient compute later.
    Args:
        input (torch.Tensor): The input tensor to the neuron.
        thrs (torch.Tensor): The threshold levels for the neuron.
        num_thrs (int): The number of threshold levels.
        out (torch.Tensor): The output tensor storing the number of spikes.
    Returns:
        out (torch.Tensor): The output tensor with the number of spikes occurred.

    """
    level_outputs = []
    Level_weight = []   
    cumulative_mask = torch.zeros_like(input)
    for level in range(num_thrs, 0, -1):
        mask = (input >= level * thrs).float()
        clean_mask = mask * (1. - cumulative_mask)
        level_outputs.append(clean_mask * level * thrs)
        Level_weight.append((level / num_thrs) * clean_mask)
        cumulative_mask = cumulative_mask + clean_mask

    out = torch.stack(level_outputs).sum(dim=0)
    weight_mask = torch.stack(Level_weight).sum(dim=0)
    return out, weight_mask



#============================= 

#===== Surrogate Gradient Function =====
# Straight-Through Estimator
class STEGradientFunction(Function):
    @staticmethod
    def forward(ctx, input, thrs, num_thrs):
        out = forward_compute(input, thrs, num_thrs)
        gradient_mask = ((input.detach() >= 0.5 * thrs) * (input.detach() <= (num_thrs + 0.5) * thrs)).float()
        # gradient_mask create a masked region with the size 
        # is the size of input expanded to +(1/2) of the threshold region on both sides.

        ctx.save_for_backward(gradient_mask)
        return out

    @staticmethod
    def backward(ctx, grad_output):  #STE gradient
        (gradient_mask,) = ctx.saved_tensors
        grad_input = grad_output * gradient_mask
        return grad_input, None, None  # None list : thrs, num_thrs

# Piecewise linear caculate
@torch.jit.script
def PiecewiseLinearCompute(input: torch.Tensor, thrs: torch.Tensor, num_thrs: int, alpha: float):
    levels = torch.arange(1, num_thrs + 1, device=input.device, dtype=input.dtype) * thrs
    dist = torch.abs(input.unsqueeze(-1) - levels)
    grad_of_level = torch.clamp(1. - dist/thrs, min=0.0) * (alpha/ num_thrs)
    grad_out = grad_of_level.sum(dim=-1)
    return grad_out

# Piecewise linear gradient function
class PiecewiseLinearGradientFunction(Function):
    @staticmethod
    def forward(ctx, input, thrs, num_thrs, alpha):
        out = forward_compute(input, thrs, num_thrs)
        ctx.save_for_backward(input)
        ctx.alpha = alpha
        ctx.thrs = thrs
        ctx.num_thrs = num_thrs
        return out

    @staticmethod
    def backward(ctx, grad_output):
        (input,) = ctx.saved_tensors
        thrs = ctx.thrs
        alpha = ctx.alpha
        num_thrs = ctx.num_thrs 
        grad_input = grad_output * PiecewiseLinearCompute(input, thrs, num_thrs, alpha)
        return grad_input, None, None, None  # None list : thrs, num_thrs, alpha


# Weighted STE use forward_compute_w_mask

class WSTEGradientFunction(Function):
    @staticmethod
    def forward(ctx, input, thrs, num_thrs):
        out, weight_mask = forward_compute_w_mask(input, thrs, num_thrs)
        gradient_mask = ((input.detach() >= 0.5 * thrs) * (input.detach() <= (num_thrs + 0.5) * thrs)).float()
        # gradient_mask create a masked region with the size 
        # is the size of input expanded to +(1/2) of the threshold region on both sides.

        ctx.save_for_backward(gradient_mask, weight_mask)
        return out

    @staticmethod
    def backward(ctx, grad_output):  #STE gradient
        (gradient_mask, weight_mask) = ctx.saved_tensors
        grad_input = grad_output * gradient_mask * weight_mask
        return grad_input, None, None  # None list : thrs, num_thrs

#===== Neuron Class =====
class LMHT_LIF(nn.Module):
    def __init__(self, threshold=1.0, Time_step=2, leakage=0., Num_thresholds: int = 4, 
                 Reset_mode='soft', V_init=0., alpha: float = 1.0, Activation='STEGradientFunction'):
        super(LMHT_LIF, self).__init__()
        self.V = None
        self.V_threshold = nn.Parameter(torch.tensor(threshold), requires_grad=False)
        self.leakage = nn.Parameter(torch.tensor(leakage), requires_grad=True) 
        self.Num_thresholds = Num_thresholds
        self.Reset_mode = Reset_mode
        self.V_init = V_init * threshold
        self.Time_step = Time_step
        self.mask = nn.Parameter(torch.zeros((Time_step, Time_step, 1, 1, 1, 1)), requires_grad=True)
        self.mask_linear = nn.Parameter(torch.zeros((Time_step, Time_step, 1, 1, 1)), requires_grad=True)
        self.scale = 1.
        self.alpha = alpha

        if Activation == 'FastSigmoid':
            raise NotImplementedError("FastSigmoid activation is not implemented.")
        elif Activation == 'PiecewiseLinearGradientFunction':
            self.Activation = PiecewiseLinearGradientFunction.apply
            self.use_alpha = True
        elif Activation == 'WSTEGradientFunction':  
            self.Activation = WSTEGradientFunction.apply
            self.use_alpha = False
        else:
            self.Activation = STEGradientFunction.apply
            self.use_alpha = False

    def reset_mechanism(self, output: torch.Tensor):
        """
        Reset the membrane potential after spikes occured.
        """
        if self.Reset_mode == 'soft':
            self.V -= output.detach()
        elif self.Reset_mode == 'hard':
            self.V = torch.full_like(self.V, self.V_init)
        
        return self.V

    def forward(self, x):
        """
        Forward pass of the neuron.
        Args:
            x (torch.Tensor): Input tensor.
        Returns:
            torch.Tensor: Output tensor after applying the neuron function.
        """
        self.V = torch.ones_like(x[0]) * self.V_init
        x = x * self.scale

        if len(x.shape) == 5:
            self.kelner = self.mask
        else:
            self.kelner = self.mask_linear

        Spikes = []
        for t in range(self.Time_step):
            # Membrane potential = V
            # Membrane potential update
            self.V = (self.leakage.sigmoid() + 0.5) * self.V.detach() + ((2 * self.kelner[t].sigmoid() / x.shape[0]) * x).sum(dim=0)
            if self.use_alpha:
                output = self.Activation(self.V, self.V_threshold, self.Num_thresholds, self.alpha)
            else:
                output = self.Activation(self.V, self.V_threshold, self.Num_thresholds)
            self.V = self.reset_mechanism(output) # reset membrane potential
            Spikes.append(output)

        return torch.stack(Spikes, dim=0)

#========================

#===== Test ======
def testSTEGradient():
    input = torch.tensor([0.3, 1.0, 2.3, 3.5, 4.7], requires_grad=True)
    th = torch.tensor(1.0)
    num_thresholds = 2

    output = STEGradientFunction.apply(input, th, num_thresholds)
    print('Output:', output)

    output.sum().backward()
    print('Gradient:', input.grad)
    print('Gradient mask:', (input.grad != 0).tolist())


def test_LMHT_LIF():
    print("\nTesting LMHT_LIF with 4 levels and 4 time steps:")
    neuron = LMHT_LIF(threshold=1.0, Time_step=4, leakage=0., Num_thresholds=4, 
                 Reset_mode='soft', V_init=0., alpha=1.0, Activation='STEGradientFunction')
    # input shape   T, B, C, H, W
    input = torch.randn(4, 64, 14, 15, 20)
    print('Input shape:', input.shape)
    # Forward pass
    output = neuron(input)
    print('Output shape:', output.shape)



def test_Piecewise_gradient():
    input = torch.tensor([0.3, 1.0, 2.3, 3.5, 4.7], requires_grad=True)
    th = torch.tensor(1.0)
    num_thresholds = 2
    alpha = 1.0
    output = PiecewiseLinearGradientFunction.apply(input, th, num_thresholds, alpha)
    print('Output:', output)

    output.sum().backward()
    print('Gradient:', input.grad)
    print('Gradient mask:', (input.grad != 0).tolist())

if __name__ == "__main__":
    testSTEGradient()
    test_Piecewise_gradient()
    test_LMHT_LIF()